## configuration file
configfile: 'config.yaml'

## output directory
outputDir=config['output']

## running entire workflow
rule all:
    input:
        os.path.join(outputDir, "quant", "kb_output")

## ------------------------------------------------------------------------------------ ##
## demultiplexing, if needed
## ------------------------------------------------------------------------------------ ##
if config['multiplex']:

    ## the goal is to demultiplex original FASTQ file into multiple donor-specific FASTQ files, one for each sample tag
    ## steps:
    ## 1. identify the correct sample tag for each cellular barcode (dataframe 1)
    ## 2. extract the read IDs associated with each cellular barcode (dataframe 2)
    ## 3. join both dataframes to match cellular barcodes with their sample tags and read IDs
    ## 4. create separate dataframes for each sample tag, each containing the read IDs for that tag
    ## 5. using the read IDs, split the original FASTQ file into multiple donor-specific FASTQ files
    
    ## sample tag tsv file preparation
    ## changing file format to be compatible with kallisto
    rule sample_tag_prep:
        input: 
            sample_tag_fasta=config['sample_tag_seqs']
        output:
            sample_tag_tsv=os.path.join(outputDir, "demultiplex", "meta", "sample_tag.tsv")
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_prep.log")
        script:
            "scripts/1_sample_tag_prep.R"

    ## building a sample tag index and t2g files
    ## these are necessary to run kallisto for sample tag quantification
    rule sample_tag_index:
        input:
            tsv=os.path.join(outputDir, "demultiplex", "meta", "sample_tag.tsv")
        output:
            index=os.path.join(outputDir, "demultiplex", "meta", "st_index.idx"),
            fasta=os.path.join(outputDir, "demultiplex", "meta", "st_mismatch.fa"),
            t2g=os.path.join(outputDir, "demultiplex", "meta", "st_f2g.txt")
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_index.log")
        shell:
            "kb ref -i {output.index} -f1 {output.fasta} -g {output.t2g} --workflow kite {input.tsv} > {log} 2>&1"

    ## sample tag quantification
    ## only the first R1 and R2 FASTQ files are used
    ## this significantly reduces both runtime and memory requirements
    ## while still accurately identifying HLA alleles
    ## including additional FASTQ files, such as those from different lanes of the same samples or donors, does not improve allele-typing accuracy
    rule sample_tag_quant:
        input:
            raw_data_fastq_list=config['raw_data_fastq_list'][0:2],
            index=os.path.join(outputDir, "demultiplex", "meta", "st_index.idx"),
            t2g=os.path.join(outputDir, "demultiplex", "meta", "st_f2g.txt")
        output:
            directory(os.path.join(outputDir, "demultiplex", "matrix"))
        params:
            tech=config['sequencing_technology']
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_quant.log")
        shell:
            "kb count -i {input.index} -g {input.t2g} -x {params.tech} -o {output} --workflow kite {input.raw_data_fastq_list} > {log} 2>&1"

    ## dataframe 1: cellular barcode + sample tag
    rule sample_tag_ident:
        input:
            dir=os.path.join(outputDir, "demultiplex", "matrix")
        output:
            df1=temp(os.path.join(outputDir, "demultiplex", "splitting", "df1.txt"))
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_ident.log")
        script:
            "scripts/2_sample_tag_ident.R"

    ## extracting cellular barcodes and their corresponding read IDs
    ## this rule is specifically designed for reads generated by BD Rhapsody
    ## to extract the correct cellular barcode for each read ID in R1 FASTQ
    ## the linker sequences between the cell label sections must be trimmed
    ## refer to BD Rhapsody's R1 read structure
    rule sample_tag_id:
        input:
            fastq1=config['raw_data_fastq_list'][0]
        output:
            cb=temp(os.path.join(outputDir, "demultiplex", "splitting", "cb.txt")),
            id=temp(os.path.join(outputDir, "demultiplex", "splitting", "id.txt"))
        params: 
            read_len=config['read_length']
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_id.log")
        run:
            shell(
                """
                zcat {input.fastq1} | awk 'NR%4==2 && length($1)=={params.read_len} {{ print substr($1, 1, 9) substr($1, 22, 9) substr($1, 44, 9) }}' > {output.cb}
                seqkit seq {input.fastq1} -m {params.read_len} -n -i > {output.id}
                """
            )

    ## dataframe 2: cellular barcode + ID
    rule sample_tag_id2:
        input:
            cb=os.path.join(outputDir, "demultiplex", "splitting", "cb.txt"),
            id=os.path.join(outputDir, "demultiplex", "splitting", "id.txt")
        output:
            df2=temp(os.path.join(outputDir, "demultiplex", "splitting", "df2.txt"))
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_id2.log")
        shell:
            "paste -d '\t' {input.cb} {input.id} > {output.df2}"

    ## sorting both dataframes to make joining easier and faster
    rule sample_tag_sort:
        input:
            df1=os.path.join(outputDir, "demultiplex", "splitting", "df1.txt"),
            df2=os.path.join(outputDir, "demultiplex", "splitting", "df2.txt")
        output:
            df1_sorted=temp(os.path.join(outputDir, "demultiplex", "splitting", "df1_sorted.txt")),
            df2_sorted=temp(os.path.join(outputDir, "demultiplex", "splitting", "df2_sorted.txt"))
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_sort.log")
        run:
            shell(
                """
                sort -t $'\t' -k1,1 {input.df2} -o {output.df2_sorted}
                sort -t $'\t' -k1,1 {input.df1} -o {output.df1_sorted}
                """
            )

    ## joining both dataframes --> dataframe: ceullar barcode + sample tag + ID
    rule sample_tag_join:
        input:
            df1_sorted=os.path.join(outputDir, "demultiplex", "splitting", "df1_sorted.txt"),
            df2_sorted=os.path.join(outputDir, "demultiplex", "splitting", "df2_sorted.txt")
        output:
            df=temp(os.path.join(outputDir, "demultiplex", "splitting", "df.txt"))
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_join.log")
        shell:
            "join -t $'\t' -1 1 -2 1 {input.df2_sorted} {input.df1_sorted} > {output.df}"

    ## creating a separate dataframe for each sample tag
    ## with each dataframe containing the read IDs associated with that specific tag
    ## the shell command uses `awk` to process the dataframe generated by the previous rule
    ## extracting the second field (read ID) and appending it to a new text file named based on the third field (sample tag)
    rule sample_tag_df:
        input:
            os.path.join(outputDir, "demultiplex", "splitting", "df.txt")
        output:
            directory(os.path.join(outputDir, "demultiplex", "splitting", "df"))
        run:
            shell(
                """
                mkdir -p {output}
                awk -F' ' '{{ print $2 >> ("{output}/" $3 "_df.txt") }}' {input}
                """
            )

    ## splitting the original FASTQ file into multiple donor-specific FASTQ files, one for each sample tag
    ## the shell command first lists all the text files in the input dataframe directory (each representing a sample tag)
    ## then uses `xargs` to run the `seqtk subseq` command in parallel across multiple threads
    ## this command extracts reads from the original R2 FASTQ file based on the read IDs in each sample tag file from previous rule
    ## the resulting FASTQ files are compressed and named based on the sample tag
    rule fastq_split:
        input: 
            fastq=config['raw_data_fastq_list'][1],
            df=os.path.join(outputDir, "demultiplex", "splitting", "df")
        output:
            directory(os.path.join(outputDir, "demultiplex", "splitting", "fastq"))
        params: 
            threads_number = config['threads_number']
        run:
            shell(
                """
                mkdir -p {output}
                ls {input.df}/*_df.txt | xargs -I {{}} -P {params.threads_number} sh -c 'file="{{}}"; basefile=$(basename "$file"); seqtk subseq {input.fastq} "$file" | gzip > {output}/"${{basefile%_*}}_R2.fastq.gz"'
                """
            )

## ------------------------------------------------------------------------------------ ##
## amplicon-based data
## ------------------------------------------------------------------------------------ ##
if not config['wta']:

    ## ------------------------------------------------------------------------------------ ##
    ## allele-typing
    ## ------------------------------------------------------------------------------------ ##
    
    ## changing cDNA sequence headers
    ## an example cDNA FASTA file from BD Rhapsody is provided in the demo folder
    ## the cDNA sequence headers are modified to facilitate the extraction of HLA sequences 
    ## and to properly generate the t2g file required for running kallisto
    rule change_headers: 
        input:
            original_fasta=config['amplicon_cDNA_fasta']
        output:
            updated_cDNA=os.path.join(outputDir, "allele_typing", "cDNA.fasta")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "change_headers.log")
        shell:
            "cat {input} | sed 's/location.*AMPLICON//' > {output}"

    ## trimming sequences on which allele-typing will be performed to 15bp
    ## this specific length is sufficient to distinguish between different HLA genes
    ## enabling precise allele-typing by capturing sequence variations such as mismatches or mutations
    ## this shell command uses `awk` to extract sequences from the FASTA file 
    ## whose headers match the HLA gene pattern given by the user
    ## the sequences are then trimmed and saved in a new FASTA file for allele-typing
    rule extract_sequences: 
        input:
            cdna_fasta=os.path.join(outputDir, "allele_typing", "cDNA.fasta")
        output:
            short_seq=os.path.join(outputDir, "allele_typing", "short_seq.fasta")
        params:
            genes=config['genes_to_be_allele_typed']
        log:
            os.path.join(outputDir, "allele_typing", "logs", "extract_sequences.log")
        run:
            shell(
                """
                fasta_file={input.cdna_fasta}
                gene_names=({params.genes})
                pattern=$(IFS='|'; echo "${{gene_names[*]}}")
                seq_data=$(awk -v pattern="$pattern" -v RS=">" -v FS="\\n" ' $1 ~ pattern {{ printf ">%s", $0 }} ' "$fasta_file")
                echo "$seq_data" | awk '/^>/ {{ header=$0; getline; sequence=substr($0, 1, 15); print header"\\n"sequence }}' > {output.short_seq}
                """
            )
            
    ## performing allele-typing
    ## refer to the shell script for documentation
    rule allele_typing:
        input:
            fastq=lambda wildcards: os.path.join(outputDir, "demultiplex", "splitting", "fastq") if config['multiplex'] else os.path.dirname(config['raw_data_fastq_list'][0]),
            short_seq=os.path.join(outputDir, "allele_typing", "short_seq.fasta"),
            script="scripts/3_allele_typing.sh"
        output:
            directory(os.path.join(outputDir, "allele_typing", "alleles"))
        params:
            threads_number=config['threads_number']
        log:
            os.path.join(outputDir, "allele_typing", "logs", "allele_typing.log")
        run:
            shell(
                """
                mkdir -p {output}
                ./{input.script} {input.fastq} {input.short_seq} {params.threads_number} {output}
                """
            )

    ## ------------------------------------------------------------------------------------ ##
    ## quantification
    ## ------------------------------------------------------------------------------------ ##
   
    ## combining typed-allele sequences into the final cDNA file
    ## before adding the sequences of the HLA typed alleles into the original cDNA file for quantification
    ## the HLA sequences already present must be removed
    ## once the original HLA sequences are cleared
    ## the newly typed allele sequences can be appended to the cDNA file
    rule final_cDNA_fasta:
        input:
            alleles_dir=os.path.join(outputDir, "allele_typing", "alleles"),
            genes_cDNA=os.path.join(outputDir, "allele_typing", "cDNA.fasta"),
            script="scripts/4_final_cDNA_amplicon.sh"
        output:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fasta"),
            lookup_table=os.path.join(outputDir, "quant", "lookup_table_HLA.csv")
        params:
            genes=config['genes_to_be_allele_typed']
        log:
            os.path.join(outputDir, "quant", "logs", "final_cDNA_fasta.log")
        run:
            shell(
                """
                toremove=({params.genes})
                printf ">%s\n" "${{toremove[@]}}" | tr -d ',' > data/toremove.txt
                ./{input.script} {input.alleles_dir} data/toremove.txt {input.genes_cDNA} {output.cDNA} {output.lookup_table}
                rm data/toremove.txt
                """
            )

    ## creating a transcript to gene (t2g) txt file
    ## this is necessary to run kallisto
    rule create_t2g: 
        input:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fasta")
        output:
            t2g=os.path.join(outputDir, "quant", "t2g.txt")
        log:
            os.path.join(outputDir, "quant", "logs", "create_t2g.log")
        run:
            shell(
                """
                file1=$(grep "^>" {input.cDNA} | sed 's/^.//')
                file2=$(cat {input.cDNA} | sed 's/|.*//' | grep "^>" | sed 's/^.//')
                paste <(echo "$file1") <(echo "$file2") > {output.t2g}
                """
            )
    
    ## creating cDNA index
    ## this is necessary to run kallisto
    rule create_index:
        input:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fasta")
        output:
            index=os.path.join(outputDir, "quant", "index.idx")
        log:
            os.path.join(outputDir, "quant", "logs", "create_index.log")
        shell:
            "kallisto index -i {output.index} {input.cDNA}"
    
    ## quantification using kallisto
    ## kallisto uses --mm (multi-mapping) parameter to 
    ## i. enhance sensitivity in detecting HLA alleles
    ## ii. enable better correlation with traditional quantification methods
    ## this parameter does not affect non-HLA gene quantification
    rule quantification:
        input:
            index=os.path.join(outputDir, "quant", "index.idx"),
            t2g=os.path.join(outputDir, "quant", "t2g.txt"),
            raw_data_fastq_list=config['raw_data_fastq_list'],
            lookup_old=os.path.join(outputDir, "quant", "lookup_table_HLA.csv")
        output:
            kb_dir=directory(os.path.join(outputDir, "quant", "kb_output")),
            lookup_new=os.path.join(outputDir, "quant", "kb_output", "counts_unfiltered", "lookup_table_HLA.csv")
        params:
            tech=config['sequencing_technology']
        log:
            os.path.join(outputDir, "quant", "logs", "quantification.log")
        shell:
            """  
            kb count -i {input.index} -g {input.t2g} -x {params.tech} -o {output.kb_dir} --mm --verbose {input.raw_data_fastq_list} > {log} 2>&1
            mv {input.lookup_old} {output.lookup_new}
            """

## ------------------------------------------------------------------------------------ ##
## whole transcriptome data
## ------------------------------------------------------------------------------------ ##
if config['wta']:

    ## ------------------------------------------------------------------------------------ ##
    ## allele-typing
    ## ------------------------------------------------------------------------------------ ##
    
    ## the goal is to perform allele-typing on user-defined HLA genes
    ## arcasHLA is the chosen tool, requiring a BAM file as input
    ## therefore, STAR is used to index the ref genome and align the reads
    ## producing a BAM file with aligned reads
    ## arcasHLA first uses this BAM file to extract reads mapping to chromosome 6
    ## note: HLA genes are encoded on chromosome 6
    ## and then uses IMGT/HLA database and EM model to identify alleles
    ## for each gene, the output is one or two alleles that best match the data

    ## unzipping reference and gtf files as well as HLA reference FASTA file
    rule gunzip:
        input:
            fasta=config['reference_genome_fasta'],
            gtf=config['reference_genome_gtf'],
            hla_ref="data/meta/hla_gen.fasta.gz"
        output:
            fasta="data/meta/dna.primary_assembly.fa",
            gtf="data/meta/gtf.gtf",
            hla_ref="data/meta/hla_gen.fasta"
        log:
            os.path.join(outputDir, "allele_typing", "logs", "gunzip.log")
        run:
            shell(
                """
                gunzip -c {input.fasta} > {output.fasta}
                gunzip -c {input.gtf} > {output.gtf}
                gunzip -c {input.hla_ref} > {output.hla_ref}
                """
            )

    ## STAR genome generation
    rule genome_generation: 
        input:
            fasta="data/meta/dna.primary_assembly.fa",
            gtf="data/meta/gtf.gtf"
        output:
            genome_dir=directory(os.path.join(outputDir, "allele_typing", "GenomeDir"))
        params: 
            threads_number=config['threads_number']
        log:
            os.path.join(outputDir, "allele_typing", "logs", "genome_generation.log")
        shell:
            "STAR --runMode genomeGenerate --genomeDir {output.genome_dir} --runThreadN {params.threads_number} --genomeSAindexNbases 14 --genomeFastaFiles {input.fasta} --sjdbGTFfile {input.gtf}"

    ## STAR alignment
    ## the shell command uses specific thresholds (30%)
    ## these thresholds are common for scRNA-seq data
    ## balancing between sensitivity and specificity
    ## offering flexibility for short and noisy scRNA-seq reads
    ## while avoiding overly strict filtering
    rule read_alignment:
        input:
            fastq=lambda wildcards: os.path.join(outputDir, "demultiplex", "splitting", "fastq") if config['multiplex'] else config['raw_data_fastq_list'][:2],
            genome_dir=os.path.join(outputDir, "allele_typing", "GenomeDir")
        output:
            star=temp(os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.bam"))
        params:
            threads_number=config['threads_number'],
            output_prefix=os.path.join(outputDir, "allele_typing", "STAR_alignment/")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "read_alignment.log")
        shell:
            "STAR --outFilterScoreMinOverLread 0.3 --outFilterMatchNminOverLread 0.3 --outSAMtype BAM Unsorted --runThreadN {params.threads_number} --genomeDir {input.genome_dir} --readFilesCommand zcat --readFilesIn {input.fastq} --outFileNamePrefix {params.output_prefix}"

    ## sorting BAM file
    rule bam_sort: 
        input:
            bam=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.bam")
        output:
            sorted_bam=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.sorted.bam")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "bam_sort.log")
        shell:
            "samtools sort {input.bam} -o {output.sorted_bam}"

    ## indexing BAM file
    rule bam_index:
        input:
            sorted_bam=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.sorted.bam")
        output:
            bam_index=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.sorted.bam.bai")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "bam_index.log")
        shell:
            "samtools index {input.sorted_bam}"

    ## extracting reads mapping to chromosome 6
    ## note: HLA genes are encoded on chromosome 6
    ## arcasHLA: step 1
    rule reads_extract:
        input:
            bam=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.sorted.bam"),
            bam_index=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.sorted.bam.bai")
        output:
            arcasHLA=directory(os.path.join(outputDir, "allele_typing", "arcasHLA_fastq"))
        params:
            is_single=config['single_end_sequencing'],
            threads_number=config['threads_number']
        log:
            os.path.join(outputDir, "allele_typing", "logs", "reads_extract.log")
        run:
            shell("arcasHLA reference --version 3.9.0 -v")
            if str(params.is_single).lower() == 'true':
                shell("arcasHLA extract {input.bam} -o {output.arcasHLA} -t {params.threads_number} -v --single")
            if str(params.is_single).lower() == 'false':
                shell("arcasHLA extract {input.bam} -o {output.arcasHLA} -t {params.threads_number} -v")

    ## performing HLA allele-typing
    ## arcasHLA: step 2
    rule allele_typing:
        input:
            chr6_fastq=os.path.join(outputDir, "allele_typing", "arcasHLA_fastq")
        output:
            arcasHLA=os.path.join(outputDir, "allele_typing", "arcasHLA_alleles", "Aligned.genotype.json")
        params:
            genes_to_be_allele_typed=config['genes_to_be_allele_typed'],
            is_single=config['single_end_sequencing'],
            threads_number=config['threads_number'],
            alleles_dir=os.path.join(outputDir, "allele_typing", "arcasHLA_alleles")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "allele_typing.log")
        run:
            if str(params.is_single).lower() == 'true':
                shell(
                    """
                    genes=$(echo {params.genes_to_be_allele_typed} | sed 's/HLA-//g; s/ /,/g')
                    arcasHLA genotype {input.chr6_fastq}/*.fq.gz -o {params.alleles_dir} -g "$genes" -t {params.threads_number} -v --single
                    """
                )
            if str(params.is_single).lower() == 'false':
                shell(
                    """
                    genes=$(echo {params.genes_to_be_allele_typed} | sed 's/HLA-//g; s/ /,/g')
                    arcasHLA genotype {input.chr6_fastq}/*.fq.gz -o {params.alleles_dir} -g "$genes" -t {params.threads_number} -v
                    """
                )

    ## changing allele headers
    ## arcasHLA reports alleles in JSON format
    ## standardizing headers to match cDNA format for sequence extraction
    ## enabling integration with reference cDNA file later
    rule allele_headers:
        input:
            genotype=os.path.join(outputDir, "allele_typing", "arcasHLA_alleles", "Aligned.genotype.json"),
            allelelist="data/meta/Allelelist.txt",
            cdna="data/meta/hla_gen.fasta",
            script="scripts/5_allele_headers.sh"
        output:
            allele_headers=temp(os.path.join(outputDir, "allele_typing", "matching_headers.txt"))
        log:
            os.path.join(outputDir, "allele_typing", "logs", "allele_headers.log")
        shell:
            "./{input.script} {input.genotype} {input.allelelist} {input.cdna} {output.allele_headers}"

    ## step 1: creating a cDNA file from the typed-allele sequences
    ## this file will be appended to the reference cDNA file
    ## filtering cDNA sequences based on matching allele headers from previous rule
    ## step 2: creating an HLA lookup table
    ## mapping each allele to its corresponding gene and functional class
    ## this table is crucial for constructing the data structure used in downstream analyses
    rule allele_fasta:
        input:
            allele_headers=os.path.join(outputDir, "allele_typing", "matching_headers.txt"),
            cdna="data/meta/hla_gen.fasta"
        output:
            allele_cdna=os.path.join(outputDir, "allele_typing", "allele_cDNA.fasta"),
            lookup_table=os.path.join(outputDir, "allele_typing", "lookup_table_HLA.csv")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "allele_fasta.log")
        shell:
            """
            awk 'NR==FNR{{a[">"$0];next}} /^>/{{f=$0 in a}} f' {input.allele_headers} {input.cdna} > {output.allele_cdna}
            echo "Allele,Gene,Function" > {output.lookup_table} && cat {output.allele_cdna} | awk '/^>/ {{ header=substr($0,2); split(header, fields, " "); gene = substr(fields[2], 1, index(fields[2], "*") - 1); print fields[2] "," "HLA-" gene}}' | awk -F, 'BEGIN {{OFS=","}} NR>0 {{if ($2=="HLA-A" || $2=="HLA-B" || $2=="HLA-C") $3="HLA_class_I"; else if ($2=="HLA-DRA" || $2=="HLA-DRB1" || $2=="HLA-DRB3" || $2=="HLA-DRB5" || $2=="HLA-DQA1" || $2=="HLA-DQB1" || $2=="HLA-DPA1" || $2=="HLA-DPB1") $3="HLA_class_II"; else if ($2=="HLA-E" || $2=="HLA-F" || $2=="HLA-G" || $2=="HLA-DMA" || $2=="HLA-DMB" || $2=="HLA-DOA" || $2=="HLA-DOB") $3="HLA_non_classical"; else if ($2=="HLA-H" || $2=="HLA-J" || $2=="HLA-K" || $2=="HLA-L") $3="HLA_non_coding"}} 1' >> {output.lookup_table}
            """

    ## ------------------------------------------------------------------------------------ ##
    ## quantification
    ## ------------------------------------------------------------------------------------ ##
    
    ## creating the reference cDNA file from the reference genome
    rule ref_cDNA_fasta:
        input:
            fasta=config['reference_genome_fasta'],
            gtf=config['reference_genome_gtf']
        output:
            index=os.path.join(outputDir, "quant", "meta", "ref_index.idx"),
            t2g=os.path.join(outputDir, "quant", "meta", "t2g.txt"),
            fasta=temp(os.path.join(outputDir, "quant", "genes_cDNA.fa"))
        log:
            os.path.join(outputDir, "quant", "logs", "ref_cDNA_fasta.log")
        shell:
            "kb ref -i {output.index} -g {output.t2g} -f1 {output.fasta} {input.fasta} {input.gtf}"

    ## combining typed-allele sequences into the reference cDNA file
    ## before adding the sequences of the HLA typed alleles into the reference cDNA file for quantification
    ## the HLA sequences already present must be removed
    ## once the original HLA sequences are cleared
    ## the newly typed allele sequences can be appended to the reference cDNA file
    rule final_cDNA_fasta:
        input:
            allele_cDNA=os.path.join(outputDir, "allele_typing", "allele_cDNA.fasta"),
            genes_cDNA=os.path.join(outputDir, "quant", "genes_cDNA.fa"),
            script="scripts/6_final_cDNA_wta.sh"
        output:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fa")
        params:
            genes=config['genes_to_be_allele_typed']
        log:
            os.path.join(outputDir, "quant", "logs", "final_cDNA_fasta.log")
        run:
            shell(
                """
                genes=$(echo {params.genes} | sed 's/ /|/g')
                ./{input.script} {input.allele_cDNA} {input.genes_cDNA} "$genes" {output.cDNA}
                """
            )

    ## creating a transcript to gene (t2g) txt file
    ## this is necessary to run kallisto
    rule create_t2g: 
        input:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fa")
        output:
            t2g=os.path.join(outputDir, "quant", "t2g.txt")
        log:
            os.path.join(outputDir, "quant", "logs", "create_t2g.log")
        shell:
            """
            grep "^>" {input.cDNA} | sed 's/^.//' | tr " " "\t" > {output.t2g}
            """
    
    ## creating cDNA index
    ## this is necessary to run kallisto
    rule create_index:
        input:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fa")
        output:
            index=os.path.join(outputDir, "quant", "index.idx")
        log:
            os.path.join(outputDir, "quant", "logs", "create_index.log")
        shell:
            "kallisto index -i {output.index} {input.cDNA}"

    ## quantification using kallisto
    ## kallisto uses --mm (multi-mapping) parameter to 
    ## i. enhance sensitivity in detecting HLA alleles
    ## ii. enable better correlation with traditional quantification methods
    ## this parameter does not affect non-HLA gene quantification
    rule quantification:
        input:
            index=os.path.join(outputDir, "quant", "index.idx"),
            t2g=os.path.join(outputDir, "quant", "t2g.txt"),
            raw_data_fastq_list=config['raw_data_fastq_list'],
            lookup_old=os.path.join(outputDir, "allele_typing", "lookup_table_HLA.csv")
        output:
            kb_dir=directory(os.path.join(outputDir, "quant", "kb_output")),
            lookup_new=os.path.join(outputDir, "quant", "kb_output", "counts_unfiltered", "lookup_table_HLA.csv")
        params:
            tech=config['sequencing_technology']
        log:
            os.path.join(outputDir, "quant", "logs", "quantification.log")
        shell:
            """  
            kb count -i {input.index} -g {input.t2g} -x {params.tech} -o {output.kb_dir} --mm --verbose {input.raw_data_fastq_list} > {log} 2>&1
            mv {input.lookup_old} {output.lookup_new}
            """

## ------------------------------------------------------------------------------------ ##
##                            Success and failure messages
## ------------------------------------------------------------------------------------ ##
onsuccess:
	print("Success! The Snakemake workflow is completed.")

onerror:
	print("Error! The Snakemake workflow aborted.")
