## configuration file
configfile: 'config.yaml'

## output directory
outputDir=config['output']

## running entire workflow
rule all:
    input:
        os.path.join(outputDir, "quant", "kb_output")

## ------------------------------------------------------------------------------------ ##
## demultiplexing, if needed
## ------------------------------------------------------------------------------------ ##
if config['multiplex']:
    
    ## sample tag tsv file preparation 
    rule sample_tag_prep:
        input: 
            fasta=config['sample_tag_seqs']
        output:
            tsv=os.path.join(outputDir, "demultiplex", "meta", "sample_tag.tsv")
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_prep.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        script:
            "scripts/1_sample_tag_prep.R"

    ## building a sample tag index
    rule sample_tag_index:
        input:
            tsv=os.path.join(outputDir, "demultiplex", "meta", "sample_tag.tsv")
        output:
            index=os.path.join(outputDir, "demultiplex", "meta", "st_index.idx"),
            fasta=os.path.join(outputDir, "demultiplex", "meta", "st_mismatch.fa"),
            t2g=os.path.join(outputDir, "demultiplex", "meta", "st_f2g.txt")
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_index.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "kb ref -i {output.index} -f1 {output.fasta} -g {output.t2g} --workflow kite {input.tsv} > {log} 2>&1"

    ## sample tag quantification
    rule sample_tag_quant:
        input:
            raw_data_fastq_list=config['raw_data_fastq_list'][0:2],
            index=os.path.join(outputDir, "demultiplex", "meta", "st_index.idx"),
            t2g=os.path.join(outputDir, "demultiplex", "meta", "st_f2g.txt")
        output:
            directory(os.path.join(outputDir, "demultiplex", "matrix"))
        params:
            tech=config['sequencing_technology']
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_quant.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "kb count -i {input.index} -g {input.t2g} -x {params.tech} -o {output} --workflow kite {input.raw_data_fastq_list} > {log} 2>&1"

    ## dataframe 1: cellular barcode + sample tag
    rule sample_tag_ident:
        input:
            dir=os.path.join(outputDir, "demultiplex", "matrix")
        output:
            df1=temp(os.path.join(outputDir, "demultiplex", "splitting", "df1.txt"))
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_ident.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        script:
            "scripts/2_sample_tag_ident.R"

    ## extracting cellular barcodes and their corresponding read IDs
    rule sample_tag_id:
        input:
            fastq1=config['raw_data_fastq_list'][0]
        output:
            cb=temp(os.path.join(outputDir, "demultiplex", "splitting", "cb.txt")),
            id=temp(os.path.join(outputDir, "demultiplex", "splitting", "id.txt"))
        params: 
            read_len=config['read_length']
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_id.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                zcat {input.fastq1} | awk 'NR%4==2 && length($1)=={params.read_len} {{ print substr($1, 1, 9) substr($1, 22, 9) substr($1, 44, 9) }}' > {output.cb}
                seqkit seq {input.fastq1} -m {params.read_len} -n -i > {output.id}
                """
            )

    ## dataframe 2: cellular barcode + ID
    rule sample_tag_id2:
        input:
            cb=os.path.join(outputDir, "demultiplex", "splitting", "cb.txt"),
            id=os.path.join(outputDir, "demultiplex", "splitting", "id.txt")
        output:
            df2=temp(os.path.join(outputDir, "demultiplex", "splitting", "df2.txt"))
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_id2.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "paste -d '\t' {input.cb} {input.id} > {output.df2}"

    ## sorting both dataframes
    rule sample_tag_sort:
        input:
            df1=os.path.join(outputDir, "demultiplex", "splitting", "df1.txt"),
            df2=os.path.join(outputDir, "demultiplex", "splitting", "df2.txt")
        output:
            df1_sorted=temp(os.path.join(outputDir, "demultiplex", "splitting", "df1_sorted.txt")),
            df2_sorted=temp(os.path.join(outputDir, "demultiplex", "splitting", "df2_sorted.txt"))
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_sort.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                sort -t $'\t' -k1,1 {input.df2} -o {output.df2_sorted}
                sort -t $'\t' -k1,1 {input.df1} -o {output.df1_sorted}
                """
            )

    ## joining both dataframes --> dataframe: ceullar barcode + sample tag + ID
    rule sample_tag_join:
        input:
            df1_sorted=os.path.join(outputDir, "demultiplex", "splitting", "df1_sorted.txt"),
            df2_sorted=os.path.join(outputDir, "demultiplex", "splitting", "df2_sorted.txt")
        output:
            df=temp(os.path.join(outputDir, "demultiplex", "splitting", "df.txt"))
        log:
            os.path.join(outputDir, "demultiplex", "logs", "sample_tag_join.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "join -t $'\t' -1 1 -2 1 {input.df2_sorted} {input.df1_sorted} > {output.df}"

    ## creating as many dataframes as sample tags; each dataframe contains IDs that belong to one specific sample tag
    rule sample_tag_df:
        input:
            os.path.join(outputDir, "demultiplex", "splitting", "df.txt")
        output:
            directory(os.path.join(outputDir, "demultiplex", "splitting", "df"))
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                mkdir -p {output}
                awk -F' ' '{{ print $2 >> ("{output}/" $3 "_df.txt") }}' {input}
                """
            )

    ## splitting the original fastq file into as many fastqs as sample tags according to the ID dataframes created in the previous rule
    rule fastq_split:
        input: 
            fastq=config['raw_data_fastq_list'][1],
            df=os.path.join(outputDir, "demultiplex", "splitting", "df")
        output:
            directory(os.path.join(outputDir, "demultiplex", "splitting", "fastq"))
        params: 
            threads_number = config['threads_number']
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                mkdir -p {output}
                ls {input.df}/*_df.txt | xargs -I {{}} -P {params.threads_number} sh -c 'file="{{}}"; basefile=$(basename "$file"); seqtk subseq {input.fastq} "$file" | gzip > {output}/"${{basefile%_*}}_R2.fastq.gz"'
                """
            )

## ------------------------------------------------------------------------------------ ##
## amplicon-based data
## ------------------------------------------------------------------------------------ ##
if not config['wta']:

    ## ------------------------------------------------------------------------------------ ##
    ## allele-typing
    ## ------------------------------------------------------------------------------------ ##
    ## changing cDNA headers
    rule change_headers: 
        input:
            original_fasta=config['amplicon_cDNA_fasta']
        output:
            updated_cDNA=os.path.join(outputDir, "allele_typing", "cDNA.fasta")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "change_headers.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "cat {input} | sed 's/location.*AMPLICON//' > {output}"

    ## trimming sequences on which allele-typing will be performed to 15bp
    rule extract_sequences: 
        input:
            cdna_fasta=os.path.join(outputDir, "allele_typing", "cDNA.fasta")
        output:
            short_seq=os.path.join(outputDir, "allele_typing", "short_seq.fasta")
        params:
            genes=config['genes_to_be_allele_typed']
        log:
            os.path.join(outputDir, "allele_typing", "logs", "extract_sequences.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                fasta_file={input.cdna_fasta}
                gene_names=({params.genes})
                pattern=$(IFS='|'; echo "${{gene_names[*]}}")
                seq_data=$(awk -v pattern="$pattern" -v RS=">" -v FS="\\n" ' $1 ~ pattern {{ printf ">%s", $0 }} ' "$fasta_file")
                echo "$seq_data" | awk '/^>/ {{ header=$0; getline; sequence=substr($0, 1, 15); print header"\\n"sequence }}' > {output.short_seq}
                """
            )
            
    ## performing allele-typing
    rule allele_typing:
        input:
            fastq=lambda wildcards: os.path.join(outputDir, "demultiplex", "splitting", "fastq") if config['multiplex'] else os.path.dirname(config['raw_data_fastq_list'][0]),
            short_seq=os.path.join(outputDir, "allele_typing", "short_seq.fasta"),
            script="scripts/3_allele_typing.sh"
        output:
            directory(os.path.join(outputDir, "allele_typing", "alleles"))
        params:
            threads_number=config['threads_number']
        log:
            os.path.join(outputDir, "allele_typing", "logs", "allele_typing.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                mkdir -p {output}
                chmod +x {input.script}
                ./{input.script} {input.fastq} {input.short_seq} {params.threads_number} {output}
                """
            )

    ## ------------------------------------------------------------------------------------ ##
    ## quantification
    ## ------------------------------------------------------------------------------------ ##
    ## adding the typed-allele sequences into our final cDNA file
    rule final_cDNA_fasta:
        input:
            alleles_dir=os.path.join(outputDir, "allele_typing", "alleles"),
            genes_cDNA=os.path.join(outputDir, "allele_typing", "cDNA.fasta"),
            script="scripts/4_final_cDNA_amplicon.sh"
        output:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fasta"),
            lookup_table=os.path.join(outputDir, "quant", "lookup_table_HLA.csv")
        params:
            genes=config['genes_to_be_allele_typed']
        log:
            os.path.join(outputDir, "quant", "logs", "final_cDNA_fasta.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                toremove=({params.genes})
                printf ">%s\n" "${{toremove[@]}}" | tr -d ',' > data/toremove.txt
                chmod +x {input.script}
                ./{input.script} {input.alleles_dir} data/toremove.txt {input.genes_cDNA} {output.cDNA} {output.lookup_table}
                rm data/toremove.txt
                """
            )

    ## creating a transcript to gene txt file
    rule create_t2g: 
        input:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fasta")
        output:
            t2g=os.path.join(outputDir, "quant", "t2g.txt")
        log:
            os.path.join(outputDir, "quant", "logs", "create_t2g.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                file1=$(grep "^>" {input.cDNA} | sed 's/^.//')
                file2=$(cat {input.cDNA} | sed 's/|.*//' | grep "^>" | sed 's/^.//')
                paste <(echo "$file1") <(echo "$file2") > {output.t2g}
                """
            )
    
    ## creating cDNA index
    rule create_index:
        input:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fasta")
        output:
            index=os.path.join(outputDir, "quant", "index.idx")
        log:
            os.path.join(outputDir, "quant", "logs", "create_index.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "kallisto index -i {output.index} {input.cDNA}"
    
    ## quantification using kallisto
    rule quantification:
        input:
            index=os.path.join(outputDir, "quant", "index.idx"),
            t2g=os.path.join(outputDir, "quant", "t2g.txt"),
            raw_data_fastq_list=config['raw_data_fastq_list'],
            lookup_old=os.path.join(outputDir, "quant", "lookup_table_HLA.csv")
        output:
            kb_dir=directory(os.path.join(outputDir, "quant", "kb_output")),
            lookup_new=os.path.join(outputDir, "quant", "kb_output", "counts_unfiltered", "lookup_table_HLA.csv")
        params:
            tech=config['sequencing_technology']
        log:
            os.path.join(outputDir, "quant", "logs", "quantification.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            """  
            kb count -i {input.index} -g {input.t2g} -x {params.tech} -o {output.kb_dir} --mm --verbose {input.raw_data_fastq_list} > {log} 2>&1
            mv {input.lookup_old} {output.lookup_new}
            """

## ------------------------------------------------------------------------------------ ##
## whole transcriptome data
## ------------------------------------------------------------------------------------ ##
if config['wta']:

    ## ------------------------------------------------------------------------------------ ##
    ## allele-typing
    ## ------------------------------------------------------------------------------------ ##
    ## unzipping reference and gtf files as well as HLA reference fasta file
    rule gunzip:
        input:
            fasta=config['reference_genome_fasta'],
            gtf=config['reference_genome_gtf'],
            hla_ref="data/meta/hla_gen.fasta.gz"
        output:
            fasta="data/meta/dna.primary_assembly.fa",
            gtf="data/meta/gtf.gtf",
            hla_ref="data/meta/hla_gen.fasta"
        log:
            os.path.join(outputDir, "allele_typing", "logs", "gunzip.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                gunzip -c {input.fasta} > {output.fasta}
                gunzip -c {input.gtf} > {output.gtf}
                gunzip -c {input.hla_ref} > {output.hla_ref}
                """
            )

    ## STAR genome generation
    rule genome_generation: 
        input:
            fasta="data/meta/dna.primary_assembly.fa",
            gtf="data/meta/gtf.gtf"
        output:
            genome_dir=directory(os.path.join(outputDir, "allele_typing", "GenomeDir"))
        params: 
            threads_number=config['threads_number']
        log:
            os.path.join(outputDir, "allele_typing", "logs", "genome_generation.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "STAR --runMode genomeGenerate --genomeDir {output.genome_dir} --runThreadN {params.threads_number} --genomeSAindexNbases 14 --genomeFastaFiles {input.fasta} --sjdbGTFfile {input.gtf}"

    ## STAR alignment
    rule read_alignment:
        input:
            fastq=lambda wildcards: os.path.join(outputDir, "demultiplex", "splitting", "fastq") if config['multiplex'] else config['raw_data_fastq_list'][:2],
            genome_dir=os.path.join(outputDir, "allele_typing", "GenomeDir")
        output:
            star=temp(os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.bam"))
        params:
            threads_number=config['threads_number'],
            output_prefix=os.path.join(outputDir, "allele_typing", "STAR_alignment/")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "read_alignment.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "STAR --outFilterScoreMinOverLread 0.3 --outFilterMatchNminOverLread 0.3 --outSAMtype BAM Unsorted --runThreadN {params.threads_number} --genomeDir {input.genome_dir} --readFilesCommand zcat --readFilesIn {input.fastq} --outFileNamePrefix {params.output_prefix}"

    ## sorting BAM file
    rule bam_sort: 
        input:
            bam=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.bam")
        output:
            sorted_bam=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.sorted.bam")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "bam_sort.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "samtools sort {input.bam} -o {output.sorted_bam}"

    ## indexing BAM file
    rule bam_index:
        input:
            sorted_bam=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.sorted.bam")
        output:
            bam_index=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.sorted.bam.bai")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "bam_index.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "samtools index {input.sorted_bam}"

    ## extracting reads mapping to chromosome 6
    rule reads_extract:
        input:
            bam=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.sorted.bam"),
            bam_index=os.path.join(outputDir, "allele_typing", "STAR_alignment", "Aligned.out.sorted.bam.bai")
        output:
            arcasHLA=directory(os.path.join(outputDir, "allele_typing", "arcasHLA_fastq"))
        params:
            is_single=config['single_end_sequencing'],
            threads_number=config['threads_number']
        log:
            os.path.join(outputDir, "allele_typing", "logs", "reads_extract.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell("arcasHLA reference --version 3.9.0 -v")
            if str(params.is_single).lower() == 'true':
                shell("arcasHLA extract {input.bam} -o {output.arcasHLA} -t {params.threads_number} -v --single")
            if str(params.is_single).lower() == 'false':
                shell("arcasHLA extract {input.bam} -o {output.arcasHLA} -t {params.threads_number} -v")

    ## performing allele-typing
    rule allele_typing:
        input:
            chr6_fastq=os.path.join(outputDir, "allele_typing", "arcasHLA_fastq")
        output:
            arcasHLA=os.path.join(outputDir, "allele_typing", "arcasHLA_alleles", "Aligned.genotype.json")
        params:
            genes_to_be_allele_typed=config['genes_to_be_allele_typed'],
            is_single=config['single_end_sequencing'],
            threads_number=config['threads_number'],
            alleles_dir=os.path.join(outputDir, "allele_typing", "arcasHLA_alleles")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "allele_typing.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            if str(params.is_single).lower() == 'true':
                shell(
                    """
                    genes=$(echo {params.genes_to_be_allele_typed} | sed 's/HLA-//g; s/ /,/g')
                    arcasHLA genotype {input.chr6_fastq}/*.fq.gz -o {params.alleles_dir} -g "$genes" -t {params.threads_number} -v --single
                    """
                )
            if str(params.is_single).lower() == 'false':
                shell(
                    """
                    genes=$(echo {params.genes_to_be_allele_typed} | sed 's/HLA-//g; s/ /,/g')
                    arcasHLA genotype {input.chr6_fastq}/*.fq.gz -o {params.alleles_dir} -g "$genes" -t {params.threads_number} -v
                    """
                )

    ## changing allele headers
    rule allele_headers:
        input:
            genotype=os.path.join(outputDir, "allele_typing", "arcasHLA_alleles", "Aligned.genotype.json"),
            allelelist="data/meta/Allelelist.txt",
            cdna="data/meta/hla_gen.fasta",
            script="scripts/5_allele_headers.sh"
        output:
            allele_headers=temp(os.path.join(outputDir, "allele_typing", "matching_headers.txt"))
        log:
            os.path.join(outputDir, "allele_typing", "logs", "allele_headers.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                chmod +x {input.script}
                ./{input.script} {input.genotype} {input.allelelist} {input.cdna} {output.allele_headers}
                """
            )

    ## creating a cDNA file from the typed-allele sequences
    rule allele_fasta:
        input:
            allele_headers=os.path.join(outputDir, "allele_typing", "matching_headers.txt"),
            cdna="data/meta/hla_gen.fasta"
        output:
            allele_cdna=os.path.join(outputDir, "allele_typing", "allele_cDNA.fasta"),
            lookup_table=os.path.join(outputDir, "allele_typing", "lookup_table_HLA.csv")
        log:
            os.path.join(outputDir, "allele_typing", "logs", "allele_fasta.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            """
            awk 'NR==FNR{{a[">"$0];next}} /^>/{{f=$0 in a}} f' {input.allele_headers} {input.cdna} > {output.allele_cdna}
            echo "Allele,Gene,Function" > {output.lookup_table} && cat {output.allele_cdna} | awk '/^>/ {{ header=substr($0,2); split(header, fields, " "); gene = substr(fields[2], 1, index(fields[2], "*") - 1); print fields[2] "," "HLA-" gene}}' | awk -F, 'BEGIN {{OFS=","}} NR>0 {{if ($2=="HLA-A" || $2=="HLA-B" || $2=="HLA-C") $3="HLA_class_I"; else if ($2=="HLA-DRA" || $2=="HLA-DRB1" || $2=="HLA-DRB3" || $2=="HLA-DRB5" || $2=="HLA-DQA1" || $2=="HLA-DQB1" || $2=="HLA-DPA1" || $2=="HLA-DPB1") $3="HLA_class_II"; else if ($2=="HLA-E" || $2=="HLA-F" || $2=="HLA-G" || $2=="HLA-DMA" || $2=="HLA-DMB" || $2=="HLA-DOA" || $2=="HLA-DOB") $3="HLA_non_classical"; else if ($2=="HLA-H" || $2=="HLA-J" || $2=="HLA-K" || $2=="HLA-L") $3="HLA_non_coding"}} 1' >> {output.lookup_table}
            """

    ## ------------------------------------------------------------------------------------ ##
    ## quantification
    ## ------------------------------------------------------------------------------------ ##
    ## creating a transcript to gene txt file & cDNA index & final cDNA
    rule ref_cDNA_fasta:
        input:
            fasta=config['reference_genome_fasta'],
            gtf=config['reference_genome_gtf']
        output:
            index=os.path.join(outputDir, "quant", "meta", "ref_index.idx"),
            t2g=os.path.join(outputDir, "quant", "meta", "t2g.txt"),
            fasta=temp(os.path.join(outputDir, "quant", "genes_cDNA.fa"))
        log:
            os.path.join(outputDir, "quant", "logs", "ref_cDNA_fasta.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "kb ref -i {output.index} -g {output.t2g} -f1 {output.fasta} {input.fasta} {input.gtf}"

    rule final_cDNA_fasta:
        input:
            allele_cDNA=os.path.join(outputDir, "allele_typing", "allele_cDNA.fasta"),
            genes_cDNA=os.path.join(outputDir, "quant", "genes_cDNA.fa"),
            script="scripts/6_final_cDNA_wta.sh"
        output:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fa")
        params:
            genes=config['genes_to_be_allele_typed']
        log:
            os.path.join(outputDir, "quant", "logs", "final_cDNA_fasta.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        run:
            shell(
                """
                genes=$(echo {params.genes} | sed 's/ /|/g')
                chmod +x {input.script}
                ./{input.script} {input.allele_cDNA} {input.genes_cDNA} "$genes" {output.cDNA}
                """
            )

    rule create_t2g: 
        input:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fa")
        output:
            t2g=os.path.join(outputDir, "quant", "t2g.txt")
        log:
            os.path.join(outputDir, "quant", "logs", "create_t2g.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            """
            grep "^>" {input.cDNA} | sed 's/^.//' | tr " " "\t" > {output.t2g}
            """

    rule create_index:
        input:
            cDNA=os.path.join(outputDir, "quant", "cDNA.fa")
        output:
            index=os.path.join(outputDir, "quant", "index.idx")
        log:
            os.path.join(outputDir, "quant", "logs", "create_index.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            "kallisto index -i {output.index} {input.cDNA}"

    ## quantification using kallisto
    ## quantification using kallisto
    rule quantification:
        input:
            index=os.path.join(outputDir, "quant", "index.idx"),
            t2g=os.path.join(outputDir, "quant", "t2g.txt"),
            raw_data_fastq_list=config['raw_data_fastq_list'],
            lookup_old=os.path.join(outputDir, "allele_typing", "lookup_table_HLA.csv")
        output:
            kb_dir=directory(os.path.join(outputDir, "quant", "kb_output")),
            lookup_new=os.path.join(outputDir, "quant", "kb_output", "counts_unfiltered", "lookup_table_HLA.csv")
        params:
            tech=config['sequencing_technology']
        log:
            os.path.join(outputDir, "quant", "logs", "quantification.log")
        resources:
            runtime=240,
            tasks=1,
            cpus_per_task=40,
            nodes=1,
            mem_mb_per_cpu=2000
        shell:
            """  
            kb count -i {input.index} -g {input.t2g} -x {params.tech} -o {output.kb_dir} --mm --verbose {input.raw_data_fastq_list} > {log} 2>&1
            mv {input.lookup_old} {output.lookup_new}
            """

## ------------------------------------------------------------------------------------ ##
##                            Success and failure messages
## ------------------------------------------------------------------------------------ ##
onsuccess:
	print("Success! The Snakemake workflow is completed.")

onerror:
	print("Error! The Snakemake workflow aborted.")
